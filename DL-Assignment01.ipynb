{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Members:\n",
    "\n",
    "- Honegger Roberto, 16-715-419\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmubxigEE-Zb"
   },
   "source": [
    "# Assignment 1: Universal Function Approximator\n",
    "\n",
    "\n",
    "The goal of this exercise is to compare three different neural network architectures and analyze their capacity for function approximation:\n",
    "\n",
    "1. $N_1$: One-layer network (linear transformation only)\n",
    "2. $N_2$: One-layer network with non-linear activation function\n",
    "3. $N_3$: Two-layer network (hidden layer with non-linear activation function)\n",
    "\n",
    "They will be trained via gradient descent (with weight decay). To show the flexibility of the approach, three different functions will be approximated:\n",
    "1. $X_1: t = \\cos(3x)$ for $x\\in[-2,2]$\n",
    "2. $X_2: t = e^{-x^2}$ for $x\\in[-1,1]$\n",
    "3. $X_3: t = x^5 + 3x^4 - 6x^3 -12x^2 + 5x + 129$ for $x\\in[-4,2.5]$\n",
    "\n",
    "In the theoretical section, the networks will be designed, and the necessary derivatives will be computed by hand.\n",
    "\n",
    "In the coding section, we will: \n",
    "\n",
    "- implement the networks and their gradients,\n",
    "- generate target data for three different functions, \n",
    "- apply the training procedure to the data, and \n",
    "- plot the resulting approximated function together with the data samples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "n36PMmPANmJ7"
   },
   "source": [
    "## Section 1: Theoretical Questions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "oXFApo7obLKe"
   },
   "source": [
    "### Network Design"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "FH8oL0-MQ0IY"
   },
   "source": [
    "#### Task 1.1: Network Structure\n",
    "\n",
    "Given input $\\vec x = (1, x)^T$, define three neural networks ($N_1$, $N_2$, $N_3$) mathematically, to reach output $y$. Use $g()$ to represent the activation function.\n",
    "\n",
    "Explain how their structures differ and analyze their function approximation capabilities.\n",
    "\n",
    "--- \n",
    "Note:\n",
    "\n",
    "For one-layer networks, define parameter $\\Theta=\\vec w \\in\\mathbb R^{D+1}$\n",
    "\n",
    "For two-layer network, define parameters $\\Theta=(\\mathbf W^{(1)},\\vec w^{(2)})$ that are split into $\\mathbf W^{(1)}\\in\\mathbb R^{K\\times {(D+1)}}$ for the first layer and $\\vec w^{(2)}\\in\\mathbb R^{K+1}$ for the second layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "kujWaTnHTEnk"
   },
   "source": [
    "#### Answer 1.1\n",
    "\n",
    "### $N_1$: One-Layer Network (Linear Transformation Only)\n",
    "\n",
    "**Parameters:**  \n",
    "Weight vector as  \n",
    "$\\vec{w} = \\begin{pmatrix} w_0 \\\\ w_1 \\end{pmatrix} \\in \\mathbb{R}^{2}.$\n",
    "\n",
    "**Mapping:**  \n",
    "Computed by the dot product of  \n",
    "$y = \\vec{w}^T \\vec{x} = w_0 + w_1 x.$\n",
    "\n",
    "**Function Approximation Capability:**  \n",
    "Since this model is linear and can only represent straight-line functions, it is limited in its ability to capture non-linear behaviors (e.g., in $\\cos(3x)$ or $e^{-x^2}$).\n",
    "\n",
    "---\n",
    "\n",
    "### $N_2$: One-Layer Network with Non-Linear Activation\n",
    "\n",
    "**Parameters:**  \n",
    "Same weight vector as in $N_1$:\n",
    "$\\vec{w} = \\begin{pmatrix} w_0 \\\\ w_1 \\end{pmatrix} \\in \\mathbb{R}^{2}.$\n",
    "\n",
    "**Mapping:**  \n",
    "1. Linear combination:\n",
    "   $a = w_0 + w_1 x.$  \n",
    "2. Then apply a non-linear activation function $g(\\cdot)$:\n",
    "   $y = g(a) = g(w_0 + w_1 x).$  \n",
    "\n",
    "The output becomes a non-linear transformation of the input.\n",
    "\n",
    "**Function Approximation Capability:**  \n",
    "The introduction of the non-linearity allows the network to model curved relationships. However, with only one non-linear unit, the model's flexibility is still somewhat limited.\n",
    "\n",
    "---\n",
    "\n",
    "### $N_3$: Two-Layer Network (Hidden Layer with Non-Linear Activation)\n",
    "\n",
    "**Parameters:**  \n",
    "- **First Layer:**  \n",
    "  Define the weight matrix for the hidden layer:\n",
    "  $\\mathbf{W}^{(1)} \\in \\mathbb{R}^{K \\times 2},$\n",
    "  which maps the input $\\vec{x}$ to $K$ hidden neurons.\n",
    "  \n",
    "- **Second Layer:**  \n",
    "  Define the weight vector for the output layer:\n",
    "  $\\vec{w}^{(2)} \\in \\mathbb{R}^{K+1},$\n",
    "  where the extra component accounts for the bias in the output layer.\n",
    "\n",
    "**Mapping:**  \n",
    "\n",
    "1. **Hidden Layer:**  \n",
    "   - Compute the pre-activation for each hidden neuron:\n",
    "     $a_k = \\bigl(\\mathbf{W}^{(1)} \\vec{x}\\bigr)_k,\\quad k = 1, \\dots, K.$  \n",
    "   - Apply the activation function element-wise:\n",
    "     $h_k = g(a_k).$  \n",
    "   - Augment the hidden layer output with a bias term:\n",
    "     $\\vec{h}_{\\text{aug}} = \\begin{pmatrix} 1 \\\\ h_1 \\\\ \\vdots \\\\ h_K \\end{pmatrix}.$\n",
    "\n",
    "2. **Output Layer:**  \n",
    "   - Compute the final output as:\n",
    "     $y = \\bigl(\\vec{w}^{(2)}\\bigr)^T \\vec{h}_{\\text{aug}} = w_0^{(2)} + \\sum_{k=1}^{K} w_k^{(2)} h_k.$\n",
    "\n",
    "**Function Approximation Capability:**  \n",
    "With a hidden layer that employs non-linear activations, $N_3$ is capable of approximating complex, non-linear functions. According to the Universal Approximation Theorem, a two-layer network with a sufficient number of hidden neurons can approximate any continuous function on a compact set, making it the most flexible among the three architectures.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "eSm46qR8SdQH"
   },
   "source": [
    "#### Task 1.2: Network Comparison\n",
    "\n",
    "Can the one-layer network approximate all three functions well? Why or why not?\n",
    "\n",
    "What advantages does the two-layer network have compared to a one-layer network?\n",
    "\n",
    "How can we determine the appropriate number of hidden neurons?\n",
    "When looking at the example plots in the OLAT, how many hidden neurons do we need in order to approximate the functions? Is there any difference between the three target functions?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "N_ZK0aMfafSP"
   },
   "source": [
    "#### Answer 1.2:\n",
    "\n",
    "##### 1. Can the one-layer network approximate all three functions well?\n",
    "\n",
    "A one-layer network (with only a linear transformation, $y = w_0 + w_1 x$) can only modelstraight-line functions. None of the target functions are linear:\n",
    "- **$X_1: t = \\cos(3x)$** is oscillatory and therefore periodic.\n",
    "- **$X_2: t = e^{-x^2}$** is a bell-shaped.\n",
    "- **$X_3: t = x^5 + 3x^4 - 6x^3 - 12x^2 + 5x + 129$** is not linear at all.\n",
    "\n",
    "Because a one-layer (purely linear) model cannot follow changing slopes, it will generally **underfit** these functions. In other words, it cannot capture the oscillations, the peaked shape, or the complex curvature required to closely follow the target functions.\n",
    "\n",
    "---\n",
    "\n",
    "##### 2. What advantages does the two-layer network have compared to a one-layer network?\n",
    "\n",
    "A two-layer network (i.e., a network with one hidden layer that uses a non-linear activation) offers significant advantages:\n",
    "- **Non-Linearity & Flexibility:**  \n",
    "  The hidden layer applies a non-linear activation function (e.g., sigmoid) to the weighted inputs. This non-linearity allows the network to create complex combinations of basis functions. As a result, it might is able to capture curves and oscillations that a simple linear model cannot.\n",
    "  \n",
    "- **Universal Approximation:**  \n",
    "  The Universal Approximation Theorem tells us that with a high enough number of hidden neurons, a two-layer network can approximate any continuous function on a compact interval arbitrarily well. This is way more powerful than a one-layer network. \n",
    "  \n",
    "- **Intermediate Representations:**  \n",
    "  The hidden layer can learn intermediate features or representations of the input. These features, when linearly combined in the output layer, yield a non-linear overall mapping from $x$ to $y$.\n",
    "\n",
    "---\n",
    "\n",
    "##### 3. How can we determine the appropriate number of hidden neurons?\n",
    "\n",
    "###### **General Considerations:**\n",
    "- **Empirical Tuning:**  \n",
    " Typically chosen by experimenting with different sizes and selecting the one that best balances the trade-off between underfitting (too few neurons) and overfitting (too many neurons).\n",
    "  \n",
    "- **Validation Performance:**  \n",
    "  WE can train the network with various numbers of hidden neurons and compare the approximation error on a validation set. A plateau or a minimal validation error suggests an appropriate network size.\n",
    "\n",
    "##### **Based on the Example Plots in OLAT:**\n",
    "- For **$X_1: \\cos(3x)$** and **$X_2: t = e^{-x^2}$**, the sample plots often indicate that a small number (e.g., around 3 to 4 hidden neurons) is sufficient to capture the main non-linear behavior.\n",
    "- For **$X_3: t = x^5 + 3x^4 - 6x^3 - 12x^2 + 5x + 129$**, the function is more complex. The plots suggest that slightly more neurons (perhaps 4 to 5 hidden units) might be necessary to adequately approximate the function.\n",
    "- \n",
    "- **Differences Between the Functions:**  \n",
    "  While all three functions are non-linear, their complexity differs. The cosine and exponential functions have smooth, regular shapes that can be approximated well with fewer neurons. In contrast, the polynomial, because of its higher degree and more variable curvature over the interval, may require a marginally higher number of neurons.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ki1cAn6zSvj2",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Task 1.3: Network Performance\n",
    "\n",
    "If the network struggles to approximate a function well, what are some possible reasons?\n",
    "\n",
    "How can we improve the network's performance?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "DP60Na-kbF_1",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Answer 1.3:\n",
    "\n",
    "Several factors might be involved when a network struggles to approximate a function. Here some reasons & potential solutions:\n",
    "\n",
    "---\n",
    "\n",
    "##### Reasons\n",
    "\n",
    "- **Insufficient Model Capacity:**  \n",
    "  The network might be too simple—e.g., having too few hidden neurons or layers—to capture the complexity of the target function. For non-linear functions like \\(\\cos(3x)\\) or a high-degree polynomial, a single-layer network is usually inadequate.\n",
    "\n",
    "- **Wrong Activation Functions:**  \n",
    "  If the chosen activation functions (like the sigmoid) saturate (i.e., their outputs are near 0 or 1 for a wide range of inputs), the gradients can vanish. This slows down learning or may even halt it altogether.\n",
    "\n",
    "- **Bad Weight Initialization:**  \n",
    "  If weights are initialized too high or too low, the network might start in a region of the loss surface where gradients are very small or erratic, making learning inefficient.\n",
    "\n",
    "- **Suboptimal Learning Rate:**  \n",
    "  A learning rate that is too high can cause the training process to overshoot minima and diverge, while a learning rate that is too low can result in very slow convergence.\n",
    "\n",
    "- **Problem with the Data:**  \n",
    "  Unnormalized or poorly scaled input data can hinder training. If the range of input values is too wide, some neurons might saturate or dominate the learning process.\n",
    "\n",
    "- **Not enough Training Data or poor Training:**  \n",
    "  The network may not have been trained on enough samples or for enough iterations, resulting in an incomplete learning process.\n",
    "\n",
    "- **Overfitting/Underfitting:**  \n",
    "  Depending on the network design and the complexity of the function, the model may underfit (if it is too simple) or overfit (if it memorizes noise rather than learning the underlying function).\n",
    "\n",
    "---\n",
    "\n",
    "##### Solutions\n",
    "\n",
    "- **Increase Model Capacity:**  \n",
    "  Add more hidden neurons or layers. A deeper network or one with more neurons in the hidden layer(s) can capture more complex patterns.\n",
    "\n",
    "- **Explore more with Activation Functions:**  \n",
    "  Consider using activation functions like ReLU or tanh, which may provide better gradient flow and less saturation than the sigmoid function.\n",
    "\n",
    "- **Improve Weight Initialization:**  \n",
    "  Utilize methods such as Xavier/Glorot initialization to start with weights that are neither too high nor too low, which helps maintain a good gradient scale during training.\n",
    "\n",
    "- **Tune the Learning Rate:**  \n",
    "  Adjust the learning rate to an optimal value; sometimes using a learning rate schedule or an adaptive method (e.g., Adam, RMSProp) can help, although in this assignment gradient descent with weight decay is used.\n",
    "\n",
    "- **Normalize the Data:**  \n",
    "  Standardize or normalize inputs (and even target outputs) so that all features contribute similarly to the learning process. This helps prevent certain neurons from saturating and improves overall training dynamics.\n",
    "\n",
    "- **Apply Regularization:**  \n",
    "  Use weight decay (already in the assignment) or consider other methods like dropout to prevent overfitting if the network is too complex relative to the amount of data.\n",
    "\n",
    "- **Increase Training Duration:**  \n",
    "  Train the network for more epochs or iterations. Sometimes the network simply needs more time to converge to a good solution.\n",
    "\n",
    "- **Monitor and Adjust:**  \n",
    "  Use validation data to monitor the network’s performance during training. Adjust hyperparameters such as the learning rate, momentum (if applicable), and network size based on validation performance.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "UPUvWGhybtv9"
   },
   "source": [
    "### Derivatives"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "F9ibD4zrPOvE",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Task 1.4: Activation Function\n",
    "\n",
    "Given the hyperbolic tangent ($\\tanh$) activation function as:\n",
    "\n",
    "$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "Prove:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial x} \\tanh(x) = 1 - \\tanh^2(x)$$\n",
    "\n",
    "Hint: Apply the derivative rules as defined in the Lecture:\n",
    "* Quotient rule\n",
    "* Sum rule\n",
    "* Exponential rule\n",
    "\n",
    "Also, avoid factoring out parentheses."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TW6QrQKPUffH"
   },
   "source": [
    "#### Answer 1.4:\n",
    "\n",
    "$\\frac{\\partial}{\\partial x} \\tanh(x) =...$\n",
    "\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}},\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "mC041GO7PzIP"
   },
   "source": [
    "#### Task 1.5: Weight Decay\n",
    "\n",
    "Consider a loss function with L2 regularization (weight decay):\n",
    "$$\n",
    "L'(\\theta) = L(\\theta) + \\frac{\\lambda}{2} \\|\\theta\\|^2\n",
    "$$\n",
    "\n",
    "Compute its derivative with respect to $\\theta$: $$\\frac{\\partial}{\\partial \\theta} L'(\\theta)$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2o4IjaZ4VimR"
   },
   "source": [
    "#### Answer 1.5:\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aOiw6bNJQG5E",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Task 1.6\n",
    "\n",
    "How large should an appropriate weight decay parameter $\\lambda$ as shown in Task 1.5 be? What would happen if $\\lambda$ is set too high or too low?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "vErgVANhV9cu",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Answer 1.6: \n",
    "\n",
    "Through expirementation you can determine an appropriate $\\lambda$. \n",
    "\n",
    "\n",
    "- **If $\\lambda$ is too high:**  \n",
    "  - The regularization term \\(\\lambda \\|\\theta\\|^2\\) will dominate the loss.\n",
    "  - The network will be forced to keep the weights very small, which can severely limit its capacity to learn the underlying patterns.\n",
    "  - This usually leads to **underfitting**, where the model is too simple to capture the complexity of the data.\n",
    "\n",
    "- **If $\\lambda$ is too low:**  \n",
    "  - The regularization effect becomes negligible.\n",
    "  - The network may then use large weights to fit the training data very closely.\n",
    "  - This increases the risk of **overfitting**, where the model captures noise in the training data and performs poorly on unseen data.\n",
    "\n",
    "So if Landa is too high the model underfits and if it's too low the model overfits. The optimal value depends on different factos like the data, complexity, model. etc. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SY8F0hc3Ttyn"
   },
   "source": [
    "## Section 2: Coding\n",
    "\n",
    "**<font color='red' size='5'>This section has to be submitted by 11:59 p.m. on Wednesday, March 12th, to be graded.</font>**\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "pLvRiQ-6NRB8"
   },
   "source": [
    "### Network Implementation\n",
    "#### Task 2.1\n",
    "\n",
    "Recall that for one-layer networks, we define parameter $\\Theta=\\vec w \\in\\mathbb R^{D+1}$, and for a two-layer network, we define parameters $\\Theta=(\\mathbf W^{(1)}\\in\\mathbb R^{K\\times {(D+1)}},\\vec w^{(2)}\\in\\mathbb R^{K+1})$.\n",
    "\n",
    "- D: The dimension of the input. In this assignment, $D = 1$ since there is only one input.\n",
    "- K: The number of hidden neurons in the first layer of the two-layer network ($N_3$)\n",
    "\n",
    "Implement a function that returns the network output for a given input $\\vec x$, parameter(s) $\\Theta$, and model_type ($N_1$, $N_2$, or $N_3$). Remember that the input of the function $\\vec x = (1, x)^T$.\n",
    "\n",
    "---\n",
    "Note:\n",
    "\n",
    "1. Use the `numpy` to implement the $\\tanh$ function.\n",
    "2. Use `numpy.concatenate` or `numpy.insert` to prepend $h_0$.\n",
    "3. Make use of `numpy.dot` to compute matrix-vector and vector-vector products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "4XOSElIwJ5BB"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def network(x, Theta, model_type):\n",
    "    \"\"\"\n",
    "    Compute the output of a neural network model.\n",
    "\n",
    "    Args:\n",
    "        x: Input vector (1, x) including bias.\n",
    "        Theta: Tuple of network parameters (W1, w2).\n",
    "        model_type: Type of model (1, 2, or 3).\n",
    "\n",
    "    Returns:\n",
    "        y: Network output.\n",
    "        h: Hidden layer output, or None.\n",
    "    \"\"\"\n",
    "\n",
    "    W1, w2 = Theta  # w2 is None if model_type is 1 or 2\n",
    "\n",
    "    if model_type == 1:\n",
    "        # One-layer network (Linear Model)\n",
    "        y = numpy.dot(W1, x)\n",
    "        return y, None # To make this consistent when model_type is 3\n",
    "\n",
    "    elif model_type == 2:\n",
    "        # One-layer network with tanh activation\n",
    "        a = numpy.dot(W1, x)\n",
    "        y = numpy.tanh(a)\n",
    "        return y, None # To make this consistent when model_type is 3\n",
    "\n",
    "    elif model_type == 3:\n",
    "        # Two-layer network with tanh activation.\n",
    "        a_ = numpy.dot(W1, x)  # a_ will have shape (K,), where K is the number of hidden neurons.\n",
    "        h_ = numpy.tanh(a_)\n",
    "        h = numpy.concatenate(([1], h_))  # h now has shape (K+1,)\n",
    "        y = numpy.dot(w2, h)\n",
    "        return y, h\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "g5XL_ohAE-Zh"
   },
   "source": [
    "#### Test 1: Sanity Check\n",
    "\n",
    "We select a specific number of hidden neurons and create the weights accordingly, using all zeros in the first layer and all ones in the second. The test case below ensures that the function from Task 1 actually returns $11$ for those weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ku3Sy5fzj8YH",
    "outputId": "45b07ac7-686d-432e-8d09-1342af087687"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N1 test passed.\n",
      "N2 test passed.\n",
      "N3 test passed.\n"
     ]
    }
   ],
   "source": [
    "# Define test parameters\n",
    "K = 20\n",
    "D = 1\n",
    "Theta_one_layer = [numpy.ones(D+1), None]    \n",
    "Theta_two_layer = [numpy.zeros((K, D+1)), numpy.ones(K+1)]  \n",
    "x = numpy.random.rand(D+1)\n",
    "\n",
    "# Sanity check for N1\n",
    "y1, _ = network(x, Theta_one_layer, 1)\n",
    "assert abs(numpy.sum(x) - y1) < 1e-6\n",
    "print(\"N1 test passed.\")\n",
    "\n",
    "# Sanity check for N2\n",
    "y2, _ = network(x, Theta_one_layer, 2)\n",
    "assert abs(numpy.tanh(numpy.sum(x)) - y2) < 1e-6\n",
    "print(\"N2 test passed.\")\n",
    "\n",
    "# Sanity check for N3\n",
    "y3, _ = network(x, Theta_two_layer, 3)\n",
    "assert abs(1.0 - y3) < 1e-6\n",
    "print(\"N3 test passed.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-fW7_KzcE-Zi"
   },
   "source": [
    "### Gradient Implementation\n",
    "#### Task 2.2: Gradient Computation\n",
    "\n",
    "\n",
    "Implementation of a function that returns the gradient as defined for a given dataset $X=\\{(\\vec x^{[n]}, t^{[n]})\\}$, given weight(s) $\\Theta$, model_type ($N_1$, $N_2$, or $N_3$), and $\\lambda$ parameter for weight decay.\n",
    "\n",
    "---\n",
    "Note:\n",
    "\n",
    "We should make sure that both parts of the gradient are computed for $N_3$ (since $\\Theta=(\\mathbf W^{(1)},\\vec w^{(2)})$ here).\n",
    "\n",
    "This is a very slow implementation. We will see how to speed this up in the next lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "3Vl_dYxcW-VD"
   },
   "outputs": [],
   "source": [
    "def compute_gradient(X, Theta, model_type, lambda_=1.):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss function with respect to the weights for each model type.\n",
    "\n",
    "    Args:\n",
    "        X: Dataset containing input-target pairs (x, t).\n",
    "        Theta: Network parameters (W1, w2).\n",
    "        model_type: Type of model (1, 2, or 3).\n",
    "        lambda_: Weight decay parameter. Default is 1.0.\n",
    "\n",
    "    Returns:\n",
    "        Gradients with respect to W1 and w2. For model_type 1 and 2, w2 is None.\n",
    "    \"\"\"\n",
    "\n",
    "    # split parameters for easier handling\n",
    "    W1, w2 = Theta  # w2 is None if model_type is 1 or 2\n",
    "\n",
    "    \n",
    "    # define gradient with respect to both parameters\n",
    "    if model_type in [1, 2]:\n",
    "        dW1 = numpy.zeros_like(W1)  # W1 is a vector of shape (D+1,)\n",
    "        dw2 = None\n",
    "    elif model_type == 3:\n",
    "        dW1 = numpy.zeros_like(W1)  # W1 has shape (K, D+1)\n",
    "        dw2 = numpy.zeros_like(w2)  # w2 has shape (K+1,)\n",
    "    \n",
    "    N = len(X)  # number of samples\n",
    "    \n",
    "    # iterate over dataset\n",
    "    for x, t in X:\n",
    "        if model_type == 1:\n",
    "            y = numpy.dot(W1, x)\n",
    "            error = y - t\n",
    "            dW1 += error * x\n",
    "            \n",
    "        elif model_type == 2:\n",
    "            a = numpy.dot(W1, x)\n",
    "            y = numpy.tanh(a)\n",
    "            error = y - t\n",
    "            dW1 += error * (1 - y**2) * x\n",
    "            \n",
    "        elif model_type == 3:\n",
    "\n",
    "            a = numpy.dot(W1, x)  # shape (K,)\n",
    "            h = numpy.tanh(a)     # shape (K,)\n",
    "            h_aug = numpy.concatenate(([1.0], h))  # shape (K+1,)\n",
    "            y = numpy.dot(w2, h_aug)\n",
    "            error = y - t\n",
    "            \n",
    "            # Gradient for the second layer weights:\n",
    "            dw2 += error * h_aug\n",
    "\n",
    "            # Backpropagate the error to the hidden layer.\n",
    "            # For each hidden neuron k (k = 1,...,K), compute the error term:\n",
    "            # delta_k = error * w2[k+1] * (1 - h[k]^2).\n",
    "            delta = error * w2[1:] * (1 - h**2)  # shape (K,)\n",
    "            # Gradient with respect to W1: outer product of delta and input x.\n",
    "            dW1 += numpy.outer(delta, x)  # dW1 shape (K, D+1)\n",
    "    \n",
    "    # Average the gradients over the dataset\n",
    "    dW1 /= N\n",
    "    if model_type == 3:\n",
    "        dw2 /= N\n",
    "    \n",
    "    # Add weight decay (L2 regularization) gradient:\n",
    "    # The derivative of (lambda/2)*||theta||^2 with respect to theta is lambda * theta.\n",
    "    dW1 += lambda_ * W1\n",
    "    if model_type == 3:\n",
    "        dw2 += lambda_ * w2\n",
    "    \n",
    "    return dW1, dw2\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "FqAMeoy5E-aG"
   },
   "source": [
    "#### Task 2.3: Gradient Descent\n",
    "\n",
    "The procedure of gradient descent is the repeated application of two steps.\n",
    "\n",
    "1. The gradient of loss $\\nabla_{\\Theta}\\mathcal J^{L_2}$ is computed based on the current value of the parameters $\\Theta$.\n",
    "2. The weights are updated by moving a small step in the direction of the negative gradient:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\Theta = \\Theta - \\eta \\nabla_{\\Theta}\\mathcal J\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "As a stopping criterion, we select the number of training epochs to be 10000.\n",
    "\n",
    "Implementation of a function that performs gradient descent for a given dataset $X$, given initial parameters $\\Theta$, a given learning rate $\\eta$, model_type ($N_1$, $N_2$, or $N_3$), and $\\lambda$ parameter for weight decay, and returns the optimized parameters $\\Theta^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "Hx6Jjs2e2CFX"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, Theta, eta, model_type, lambda_=1.):\n",
    "\n",
    "    epochs = 10000\n",
    "\n",
    "    # Perform iterative gradient descent.\n",
    "    for _ in range(epochs):\n",
    "        # compute the gradient\n",
    "        dW1, dw2 = compute_gradient(X, Theta, model_type, lambda_)\n",
    "        \n",
    "        # update the parameters\n",
    "        Theta[0] = Theta[0] - eta * dW1\n",
    "        \n",
    "        # return optimized parameters\n",
    "        if model_type == 3:\n",
    "            Theta[1] = Theta[1] - eta * dw2\n",
    "\n",
    "    return Theta\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5gEeh7N8E-aH"
   },
   "source": [
    "### Datasets\n",
    "\n",
    "#### Task 2.4: Data Samples\n",
    "\n",
    "In total, we will test our gradient descent function with three different datasets. Particularly, we approximate\n",
    "\n",
    "1. $X_1: t = \\cos(3x)$ for $x\\in[-2,2]$\n",
    "2. $X_2: t = e^{-x^2}$ for $x\\in[-1,1]$\n",
    "3. $X_3: t = x^5 + 3x^4 - 6x^3 -12x^2 + 5x + 129$ for $x\\in[-4,2.5]$\n",
    "\n",
    "Generate dataset $X_1$,  for $N=60$ samples randomly drawn from range $x\\in[-2,2]$. Generate data $X_2$ for $N=50$ samples randomly drawn from range $x\\in[-1,1]$. Generate dataset $X_3$ for $N=200$ samples randomly drawn from range $x\\in[-4,2.5]$. Implement all three datasets as lists of tuples: $\\{(\\vec x^{[n]}, t^{[n]})\\mid 1\\leq n\\leq N\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "1DdPBymdcNXx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 samples from X1:\n",
      "x = [1.         0.65564957] , t = -0.385871572887995\n",
      "x = [ 1.         -0.89601152] , t = -0.8988937612039112\n",
      "x = [1.         0.01213292] , t = 0.9993376382898921\n",
      "\n",
      "First 3 samples from X2:\n",
      "x = [1.         0.91684923] , t = 0.431446174019499\n",
      "x = [ 1.         -0.18187945] , t = 0.9674610309847423\n",
      "x = [ 1.         -0.06650448] , t = 0.9955869203931977\n",
      "\n",
      "First 3 samples from X3:\n",
      "x = [1.         1.58695328] , t = 111.82651098124337\n",
      "x = [1.         0.49505647] , t = 128.01626743355095\n",
      "x = [ 1.         -1.21257037] , t = 119.85464910877883\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "# X1:\n",
    "N1 = 60\n",
    "X1 = []\n",
    "xs1 = numpy.random.uniform(-2, 2, N1)\n",
    "for x in xs1:\n",
    "    x_vec = numpy.array([1.0, x])  # Input vector with bias: (1, x)^T\n",
    "    t_val = numpy.cos(3 * x)       # Target value: t = cos(3x)\n",
    "    X1.append((x_vec, t_val))\n",
    "\n",
    "# X2: \n",
    "N2 = 50\n",
    "X2 = []\n",
    "xs2 = numpy.random.uniform(-1, 1, N2)\n",
    "for x in xs2:\n",
    "    x_vec = numpy.array([1.0, x])\n",
    "    t_val = numpy.exp(-x**2)       # Target value: t = e^(-x^2)\n",
    "    X2.append((x_vec, t_val))\n",
    "\n",
    "# X3: \n",
    "N3 = 200\n",
    "X3 = []\n",
    "xs3 = numpy.random.uniform(-4, 2.5, N3)\n",
    "for x in xs3:\n",
    "    x_vec = numpy.array([1.0, x])\n",
    "    t_val = x**5 + 3*x**4 - 6*x**3 - 12*x**2 + 5*x + 129\n",
    "    X3.append((x_vec, t_val))\n",
    "\n",
    "# Optional: Display the first few samples from each dataset to verify the generation\n",
    "print(\"First 3 samples from X1:\")\n",
    "for sample in X1[:3]:\n",
    "    print(\"x =\", sample[0], \", t =\", sample[1])\n",
    "\n",
    "print(\"\\nFirst 3 samples from X2:\")\n",
    "for sample in X2[:3]:\n",
    "    print(\"x =\", sample[0], \", t =\", sample[1])\n",
    "\n",
    "print(\"\\nFirst 3 samples from X3:\")\n",
    "for sample in X3[:3]:\n",
    "    print(\"x =\", sample[0], \", t =\", sample[1])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "6tGZqaiUE-aH"
   },
   "source": [
    "#### Test 2: Sanity Check\n",
    "\n",
    "The test case below ensures that the elements of each generated dataset are tuples with two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WrneyBJLE-aI",
    "outputId": "96be9dcb-0709-45d3-b209-5a226b429045"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "assert all(\n",
    "    isinstance(x, (tuple,list)) and\n",
    "    len(x) == 2 and\n",
    "    isinstance(x[0], (tuple,list,numpy.ndarray)) and\n",
    "    len(x[0] == 2) and\n",
    "    isinstance(x[1], float)\n",
    "    for X in (X1, X2, X3)\n",
    "    for x in X\n",
    ")\n",
    "\n",
    "print('Test passed!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "v0p9mC4YE-aI"
   },
   "source": [
    "### Function Approximation\n",
    "Finally, we want to make use of our gradient descent implementation to approximate our functions. In order to see our success, we want to plot the functions together with the data.\n",
    "\n",
    "#### Task 2.5: Define hidden Neurons\n",
    "How many hidden neurons will we need for $N_3$? Use the answers from Task 1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "OLichgq7cNXy"
   },
   "outputs": [],
   "source": [
    "# Define the number of hidden neurons for each target function:\n",
    "# For X1 (t = cos(3x)), a smooth oscillatory function, ~3-4 neurons are sufficient.\n",
    "K1 = 4\n",
    "\n",
    "# For X2 (t = exp(-x^2)), a bell-shaped smooth function, ~3-4 neurons are also sufficient.\n",
    "K2 = 4\n",
    "\n",
    "# For X3 (a 5th-degree polynomial), which is more complex, we may need ~4-5 neurons.\n",
    "K3 = 5\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1fvx31eyE-aI"
   },
   "source": [
    "#### Task 2.6: Random Parameters\n",
    "\n",
    "For each of the networks, randomly initialize the parameters $\\Theta_1,\\Theta_2,\\Theta_3\\in[-1,1]$ for each of the datasets.\n",
    "\n",
    "For $N_3$, use the number of hidden neurons estimated in Task 1.2 and implemented in Task 2.5.\n",
    "\n",
    "---\n",
    "Note:\n",
    "\n",
    "  1. You can use `numpy.random.uniform` to initialize the weights.\n",
    "  2. Make sure that the weight matrices are instantiated in the correct dimensions.\n",
    "  3. Theta should always have two elements. The second element can be `None` for one-layer networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "Aq768_chcNXy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta_N1['X1'] shape: (2,)\n",
      "Theta_N2['X1'] shape: (2,)\n",
      "Theta_N3['X1'] first layer shape: (4, 2)\n",
      "Theta_N3['X1'] second layer shape: (5,)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "# For one-layer networks, D = 1 so each weight vector has shape (D+1,) = (2,)\n",
    "# and the second element is set to None.\n",
    "Theta_N1 = {\n",
    "    \"X1\": [numpy.random.uniform(-1, 1, (2,)), None],\n",
    "    \"X2\": [numpy.random.uniform(-1, 1, (2,)), None],\n",
    "    \"X3\": [numpy.random.uniform(-1, 1, (2,)), None]\n",
    "}\n",
    "\n",
    "Theta_N2 = {\n",
    "    \"X1\": [numpy.random.uniform(-1, 1, (2,)), None],\n",
    "    \"X2\": [numpy.random.uniform(-1, 1, (2,)), None],\n",
    "    \"X3\": [numpy.random.uniform(-1, 1, (2,)), None]\n",
    "}\n",
    "\n",
    "# For the two-layer network (N3), we need to choose the number of hidden neurons.\n",
    "# Based on our earlier discussion, we set:\n",
    "#   K1 = 4 for X1,\n",
    "#   K2 = 4 for X2,\n",
    "#   K3 = 5 for X3.\n",
    "K1 = 4\n",
    "K2 = 4\n",
    "K3 = 5\n",
    "\n",
    "# For N3, the first layer weight matrix should have shape (K, D+1) and the second layer weight vector has shape (K+1,).\n",
    "Theta_N3 = {\n",
    "    \"X1\": [numpy.random.uniform(-1, 1, (K1, 2)), np.random.uniform(-1, 1, (K1 + 1,))],\n",
    "    \"X2\": [numpy.random.uniform(-1, 1, (K2, 2)), np.random.uniform(-1, 1, (K2 + 1,))],\n",
    "    \"X3\": [numpy.random.uniform(-1, 1, (K3, 2)), np.random.uniform(-1, 1, (K3 + 1,))]\n",
    "}\n",
    "\n",
    "# Optionally, print out the shapes to verify\n",
    "print(\"Theta_N1['X1'] shape:\", Theta_N1[\"X1\"][0].shape)\n",
    "print(\"Theta_N2['X1'] shape:\", Theta_N2[\"X1\"][0].shape)\n",
    "print(\"Theta_N3['X1'] first layer shape:\", Theta_N3[\"X1\"][0].shape)\n",
    "print(\"Theta_N3['X1'] second layer shape:\", Theta_N3[\"X1\"][1].shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1jCM2nyRE-aI"
   },
   "source": [
    "#### Task 2.7: Run Gradient Descent\n",
    "\n",
    "For each network, call gradient descent function from Task 2.3 using the datasets $X_1, X_2, X_3$, the according created parameters $\\Theta_1,\\Theta_2,\\Theta_3$. Store the resulting optimized weights $\\Theta_1^*, \\Theta_2^*, \\Theta_3^*$.\n",
    "\n",
    "Based on your chosen learning rates $\\eta$ and weight decay parameter $\\lambda$, you may need to optimize them for these functions. Do you see any differences? What are the best learning rates that you can find?\n",
    "\n",
    "---\n",
    "<span style=\"color:red\">WARNING: Depending on the implementation, this might run for several minutes!</span>\n",
    "\n",
    "---\n",
    "Note:\n",
    "\n",
    "1. Start with $\\eta=0.1$ and play around with the learning rate improve adaptation.\n",
    "2. $\\eta=0.1$ is too large for $X_3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "I4sjPUH_cNXz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Theta for N1 on X1: [array([-0.00129397, -0.02407945]), None]\n",
      "Optimized Theta for N1 on X2: [array([0.38433499, 0.02191576]), None]\n",
      "Optimized Theta for N1 on X3: [array([62.04948081, -8.24716932]), None]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# N1\n",
    "# Call gradient descent function using the datasets and initial weights that you created above\n",
    "# Choose appropriate learning rates for each function\n",
    "\n",
    "# For Network N1 (one-layer linear model)\n",
    "eta_N1_X1 = 0.1\n",
    "eta_N1_X2 = 0.1\n",
    "eta_N1_X3 = 0.01  # Lower learning rate for the more complex X3\n",
    "\n",
    "Theta_N1_X1_star = gradient_descent(X1, Theta_N1[\"X1\"], eta_N1_X1, model_type=1, lambda_=1.0)\n",
    "Theta_N1_X2_star = gradient_descent(X2, Theta_N1[\"X2\"], eta_N1_X2, model_type=1, lambda_=1.0)\n",
    "Theta_N1_X3_star = gradient_descent(X3, Theta_N1[\"X3\"], eta_N1_X3, model_type=1, lambda_=1.0)\n",
    "\n",
    "print(\"Optimized Theta for N1 on X1:\", Theta_N1_X1_star)\n",
    "print(\"Optimized Theta for N1 on X2:\", Theta_N1_X2_star)\n",
    "print(\"Optimized Theta for N1 on X3:\", Theta_N1_X3_star)\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "Uio-gjhi3GBx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Theta for N2 on X1: [array([-0.00136801, -0.02407865]), None]\n",
      "Optimized Theta for N2 on X2: [array([0.36634883, 0.02221976]), None]\n",
      "Optimized Theta for N2 on X3: [array([ 2.60929034, -0.07671296]), None]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# N2\n",
    "# Call gradient descent function using the datasets and initial weights that you created above\n",
    "# Choose appropriate learning rates for each function\n",
    "eta_N2_X1 = 0.1\n",
    "eta_N2_X2 = 0.1\n",
    "eta_N2_X3 = 0.01  # Lower learning rate for X3\n",
    "\n",
    "Theta_N2_X1_star = gradient_descent(X1, Theta_N2[\"X1\"], eta_N2_X1, model_type=2, lambda_=1.0)\n",
    "Theta_N2_X2_star = gradient_descent(X2, Theta_N2[\"X2\"], eta_N2_X2, model_type=2, lambda_=1.0)\n",
    "Theta_N2_X3_star = gradient_descent(X3, Theta_N2[\"X3\"], eta_N2_X3, model_type=2, lambda_=1.0)\n",
    "\n",
    "print(\"Optimized Theta for N2 on X1:\", Theta_N2_X1_star)\n",
    "print(\"Optimized Theta for N2 on X2:\", Theta_N2_X2_star)\n",
    "print(\"Optimized Theta for N2 on X3:\", Theta_N2_X3_star)\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "dCPF-Q3C3GJk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Theta for N3 on X1: [array([[ 2.e-323,  2.e-323],\n",
      "       [-2.e-323, -2.e-323],\n",
      "       [ 2.e-323,  2.e-323],\n",
      "       [-2.e-323, -2.e-323]]), array([-2.85473679e-003, -1.97626258e-323,  1.97626258e-323,\n",
      "       -1.97626258e-323,  1.97626258e-323])]\n",
      "Optimized Theta for N3 on X2: [array([[ 3.5e-323,  2.0e-323],\n",
      "       [-3.5e-323, -2.0e-323],\n",
      "       [-3.5e-323, -2.0e-323],\n",
      "       [ 3.5e-323,  2.0e-323]]), array([ 3.85520656e-001,  3.45845952e-323, -3.45845952e-323,\n",
      "       -3.45845952e-323,  3.45845952e-323])]\n",
      "Optimized Theta for N3 on X3: [array([[ 3.01020765, -0.10102487],\n",
      "       [ 3.01020765, -0.10102487],\n",
      "       [-3.01020765,  0.10102487],\n",
      "       [ 3.01020765, -0.10102487],\n",
      "       [ 3.01020765, -0.10102487]]), array([ 18.47943707,  18.39741337,  18.39741337, -18.39741337,\n",
      "        18.39741337,  18.39741337])]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# N3\n",
    "# Call gradient descent function using the datasets and initial weights that you created above\n",
    "# Choose appropriate learning rates for each function\n",
    "eta_N3_X1 = 0.1\n",
    "eta_N3_X2 = 0.1\n",
    "eta_N3_X3 = 0.01  # Lower learning rate for X3\n",
    "\n",
    "Theta_N3_X1_star = gradient_descent(X1, Theta_N3[\"X1\"], eta_N3_X1, model_type=3, lambda_=1.0)\n",
    "Theta_N3_X2_star = gradient_descent(X2, Theta_N3[\"X2\"], eta_N3_X2, model_type=3, lambda_=1.0)\n",
    "Theta_N3_X3_star = gradient_descent(X3, Theta_N3[\"X3\"], eta_N3_X3, model_type=3, lambda_=1.0)\n",
    "\n",
    "print(\"Optimized Theta for N3 on X1:\", Theta_N3_X1_star)\n",
    "print(\"Optimized Theta for N3 on X2:\", Theta_N3_X2_star)\n",
    "print(\"Optimized Theta for N3 on X3:\", Theta_N3_X3_star)\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8_C3TseE-aJ"
   },
   "source": [
    "### Data and Function Plotting\n",
    "\n",
    "### Task 2.8: Plotting Function\n",
    "\n",
    "Implement a plotting function that takes a given dataset $X$, given parameters $\\Theta$, model_type, and a defined range $R=[\\min,\\max]$. Each data sample $(x^{[n]},t^{[n]})$ of the dataset is plotted as an $''x''$. In order to plot the function that is approximated by the network, generate sufficient equally-spaced input values $x\\in R$, compute the network output $y$ for these inputs, and plot them with a line.\n",
    "\n",
    "---\n",
    "Note:\n",
    "\n",
    "  1. The dataset $X$ is defined as above, a list of tuples $(\\vec x, t)$.\n",
    "  2. Each input in the dataset is defined as $\\vec x = (1,x)^T$.\n",
    "  3. Equidistant points can be obtained via `numpy.arange`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "6sII26VJcNXz"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import numpy\n",
    "\n",
    "def plot(X, Theta, model_type, R):\n",
    "\n",
    "    # Extract x values and targets from the dataset X.\n",
    "    xs_data = numpy.array([sample[0][1] for sample in X]) \n",
    "    ts_data = numpy.array([sample[1] for sample in X])\n",
    "    \n",
    "    # first, plot data samples\n",
    "    pyplot.plot(xs_data, ts_data, \"rx\", label=\"Data\")\n",
    "    \n",
    "    # Define equidistant points from R[0] to R[1].\n",
    "    step = (R[1] - R[0]) / 200.0  # adjust number of points as needed; here, ~200 points.\n",
    "    xs = numpy.arange(R[0], R[1] + step, step)\n",
    "    \n",
    "    # compute the network outputs for these values\n",
    "    # Each input to the network is defined as a column vector: (1, x)^T.\n",
    "    ys = numpy.array([network(np.array([1.0, x]), Theta, model_type)[0] for x in xs])\n",
    "    \n",
    "    # plot network approximation\n",
    "    pyplot.plot(xs,ys,\"k-\", label=\"network\")\n",
    "    pyplot.legend()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YXcO5e-sE-aJ"
   },
   "source": [
    "#### Task 2.9: Plot Three Functions\n",
    "\n",
    "For each of the datasets and their optimized parameters, call the plotting function from Task 2.8. Use range $R=[-3,3]$ for dataset $X_1$, range $R=[-2,2]$ for $X_2$, and range $R=[-5,4]$ for dataset $X_3$.\n",
    "\n",
    "Note that the first element of range $R$ should be the lowest $x$-location, and the second element of $R$ is the highest value for $x$.\n",
    "\n",
    "Repeat for three networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "CY3YGkXgcNXz"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "figure = pyplot.figure(figsize=(10, 3))\n",
    "\n",
    "# plot first function\n",
    "pyplot.subplot(131)\n",
    "plot(X1, Theta_N3_X1_star, model_type=3, R=[-3, 3])\n",
    "pyplot.title(\"N3 on X1\")\n",
    "\n",
    "# plot second function\n",
    "pyplot.subplot(132)\n",
    "plot(X2, Theta_N3_X2_star, model_type=3, R=[-2, 2])\n",
    "pyplot.title(\"N3 on X2\")\n",
    "\n",
    "# plot third function\n",
    "pyplot.subplot(133)\n",
    "plot(X3, Theta_N3_X3_star, model_type=3, R=[-5, 4])\n",
    "pyplot.title(\"N3 on X3\")\n",
    "\n",
    "pyplot.tight_layout()\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
